# Local Speech-to-Text MCP Server

A high-performance Model Context Protocol (MCP) server providing local speech-to-text transcription using whisper.cpp, optimized for Apple Silicon.

## ğŸ¯ Features

- **ğŸ  100% Local Processing**: No cloud APIs, complete privacy
- **ğŸš€ Apple Silicon Optimized**: 15x+ real-time transcription speed
- **ğŸ¤ Speaker Diarization**: Identify and separate multiple speakers
- **ğŸ“ Multiple Output Formats**: txt, json, vtt, srt, csv
- **ğŸ’¾ Low Memory Footprint**: <2GB memory usage
- **ğŸ”§ TypeScript**: Full type safety and modern development

## ğŸš€ Quick Start

### Prerequisites

- Node.js 18+
- whisper.cpp (`brew install whisper-cpp`)
- **For speaker diarization**: Python 3.8+ and HuggingFace token (free)

### Installation

```bash
git clone https://github.com/your-username/local-stt-mcp.git
cd local-stt-mcp/mcp-server
npm install
npm run build

# Download whisper models
npm run setup:models

# For speaker diarization, set HuggingFace token
export HF_TOKEN="your_token_here"  # Get free token from huggingface.co
```

**Speaker Diarization Note**: Requires HuggingFace account and accepting [pyannote/speaker-diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1) license.

### MCP Client Configuration

Add to your MCP client configuration:

```json
{
  "mcpServers": {
    "whisper-mcp": {
      "command": "node",
      "args": ["path/to/local-stt-mcp/mcp-server/dist/index.js"]
    }
  }
}
```

## ğŸ› ï¸ Available Tools

| Tool | Description |
|------|-------------|
| `transcribe` | Basic audio transcription |
| `transcribe_long` | Long audio file processing with chunking |
| `transcribe_with_speakers` | Speaker diarization and transcription |
| `list_models` | Show available whisper models |
| `health_check` | System diagnostics |
| `version` | Server version information |

## ğŸ“Š Performance

**Apple Silicon Benchmarks:**
- **Processing Speed**: 15.8x real-time (vs WhisperX 5.5x)
- **Memory Usage**: <2GB (vs WhisperX ~4GB)
- **GPU Acceleration**: âœ… Apple Neural Engine
- **Setup**: Medium complexity but superior performance

See `/benchmarks/` for detailed performance comparisons.

## ğŸ—ï¸ Project Structure

```
mcp-server/
â”œâ”€â”€ src/                    # TypeScript source code
â”‚   â”œâ”€â”€ tools/             # MCP tool implementations
â”‚   â”œâ”€â”€ whisper/           # whisper.cpp integration
â”‚   â”œâ”€â”€ utils/             # Speaker diarization & utilities
â”‚   â””â”€â”€ types/             # Type definitions
â”œâ”€â”€ dist/                  # Compiled JavaScript
â””â”€â”€ python/                # Python dependencies
```

## ğŸ”§ Development

```bash
# Build
npm run build

# Development mode (watch)
npm run dev

# Linting & formatting
npm run lint
npm run format

# Type checking
npm run type-check
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

- [whisper.cpp](https://github.com/ggerganov/whisper.cpp) for optimized inference
- [OpenAI Whisper](https://github.com/openai/whisper) for the original models
- [Model Context Protocol](https://modelcontextprotocol.io/) for the framework
- [Pyannote.audio](https://github.com/pyannote/pyannote-audio) for speaker diarization